{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d336d6e3",
   "metadata": {},
   "source": [
    "Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8082910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def positional_encoding(max_len, d_model):\n",
    "    # max_len: độ dài tối đa của 1 chuỗi\n",
    "    # d_model: kích thước embedding\n",
    "    pe = np.zeros((max_len, d_model))\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            angle = pos / (10000 ** (i / d_model))\n",
    "            # Vị trí chẵn\n",
    "            pe[pos, i] = math.sin(angle)\n",
    "            # Vị trí lẻ\n",
    "            pe[pos, i+1] = math.cos(angle)\n",
    "    return pe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f751a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma trận ban đầu:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "------------------\n",
      "ma trận positional encoding:\n",
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.84147098  0.54030231  0.00999983  0.99995   ]\n",
      " [ 0.90929743 -0.41614684  0.01999867  0.99980001]\n",
      " [ 0.14112001 -0.9899925   0.0299955   0.99955003]\n",
      " [-0.7568025  -0.65364362  0.03998933  0.99920011]]\n",
      "--------------------------\n",
      "vector PE cho vị trí thứ: 0 là:\n",
      "[0. 1. 0. 1.]\n",
      "------------------------------\n",
      "vector PE cho vị trí thứ: 1 là:\n",
      "[0.84147098 0.54030231 0.00999983 0.99995   ]\n",
      "------------------------------\n",
      "vector PE cho vị trí thứ: 2 là:\n",
      "[ 0.90929743 -0.41614684  0.01999867  0.99980001]\n",
      "------------------------------\n",
      "vector PE cho vị trí thứ: 3 là:\n",
      "[ 0.14112001 -0.9899925   0.0299955   0.99955003]\n",
      "------------------------------\n",
      "vector PE cho vị trí thứ: 4 là:\n",
      "[-0.7568025  -0.65364362  0.03998933  0.99920011]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "max_len = 5\n",
    "d_model = 4\n",
    "pe = np.zeros((5, 4))\n",
    "print(f\"ma trận ban đầu:\\n{pe}\")\n",
    "print(f\"------------------\")\n",
    "for pos in range(max_len):\n",
    "    for i in range(0, d_model, 2):\n",
    "        angle = pos / (10000 ** (i / d_model))\n",
    "        pe[pos][i] = math.sin(angle)\n",
    "        pe[pos][i+1] = math.cos(angle)\n",
    "print(f\"ma trận positional encoding:\\n{pe}\")\n",
    "print(f\"--------------------------\")\n",
    "for i in range(max_len):\n",
    "    print(f\"vector PE cho vị trí thứ: {i} là:\\n{pe[i]}\")\n",
    "    print(f\"------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51dc3c",
   "metadata": {},
   "source": [
    "Sau khi tính PE, ta cộng vào embedding ban đầu: X' = X + PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61a83c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector embedding của input:\n",
      "[[ 0.62113602  2.06516305 -1.37436111 -0.66050378]\n",
      " [-0.57817703  0.84638542  0.37200656 -0.6383473 ]\n",
      " [ 0.94573413 -1.80154467  0.71740834  0.83773453]\n",
      " [ 0.56026209 -1.22670534 -0.06983493  0.19584023]\n",
      " [ 0.47823442  1.20625307  0.73981294  0.1984902 ]]\n",
      "---------------------------------\n",
      "vector x khi thêm positional encoding:\n",
      "[[ 0.62113602  3.06516305 -1.37436111  0.33949622]\n",
      " [ 0.26329395  1.38668772  0.38200639  0.3616027 ]\n",
      " [ 1.85503155 -2.21769151  0.73740701  1.83753453]\n",
      " [ 0.7013821  -2.21669783 -0.03983943  1.19539027]\n",
      " [-0.27856808  0.55260945  0.77980227  1.19769031]]\n"
     ]
    }
   ],
   "source": [
    "# Giả sử vector X ban đầu có giá trị:\n",
    "x = np.random.randn(max_len, d_model)\n",
    "print(f\"vector embedding của input:\\n{x}\")\n",
    "x = x + pe\n",
    "print(f\"---------------------------------\")\n",
    "print(f\"vector x khi thêm positional encoding:\\n{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffcdde",
   "metadata": {},
   "source": [
    "Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf922d",
   "metadata": {},
   "source": [
    "Residual and Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "013da2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Layer normalization\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # gamma và beta\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased= False)\n",
    "\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_hat + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0d09530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma1:\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], requires_grad=True)\n",
      "gamma2:\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "---------------------------\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "d_model = 5\n",
    "gamma1 = nn.Parameter(torch.zeros(d_model))\n",
    "gamma2 = torch.zeros(d_model)\n",
    "print(f\"gamma1:\\n{gamma1}\")\n",
    "print(f\"gamma2:\\n{gamma2}\")\n",
    "print(f\"---------------------------\")\n",
    "matrix = torch.zeros(3,4)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b39125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7778, -0.3200, -0.1645, -1.4031,  0.7105],\n",
      "         [ 0.0943, -1.0420,  0.4630, -0.3948, -0.2294],\n",
      "         [ 0.6182, -0.3394,  0.1782, -2.8626, -0.3558]],\n",
      "\n",
      "        [[-2.0792,  0.7995,  0.6266,  0.9787, -0.8089],\n",
      "         [ 0.6942,  1.6814,  1.1254,  0.7986,  1.4140],\n",
      "         [ 1.3712, -0.9537,  0.5689, -0.7108, -0.6038]]])\n",
      "Trung bình của từng hàng: \n",
      "tensor([[[-0.0799],\n",
      "         [-0.2218],\n",
      "         [-0.5523]],\n",
      "\n",
      "        [[-0.0967],\n",
      "         [ 1.1427],\n",
      "         [-0.0656]]])\n",
      "Phương sai của từng hàng:\n",
      "tensor([[[0.6352],\n",
      "         [0.2543],\n",
      "         [1.4650]],\n",
      "\n",
      "        [[1.3841],\n",
      "         [0.1367],\n",
      "         [0.7923]]])\n"
     ]
    }
   ],
   "source": [
    "# Tạo input gồm 2 câu batch_size = 2\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 5\n",
    "x = torch.randn(batch_size,seq_len, d_model)\n",
    "print(x)\n",
    "mean = x.mean(dim=-1, keepdim=True)\n",
    "print(f\"Trung bình của từng hàng: \\n{mean}\")\n",
    "var = x.var(dim = -1, keepdim=True, unbiased=False)\n",
    "print(f\"Phương sai của từng hàng:\\n{var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7337264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma:\n",
      "tensor([-0.0967, -0.8902,  0.3538,  1.0674,  3.1487])\n",
      "Chuẩn hóa các giá trị của input:\n",
      "tensor([[[ 1.0761, -0.3014, -0.1061, -1.6603,  0.9917],\n",
      "         [ 0.6267, -1.6264,  1.3579, -0.3431, -0.0151],\n",
      "         [ 0.9670,  0.1759,  0.6035, -1.9088,  0.1623]],\n",
      "\n",
      "        [[-1.6852,  0.7618,  0.6148,  0.9140, -0.6054],\n",
      "         [-1.2129,  1.4568, -0.0467, -0.9307,  0.7336],\n",
      "         [ 1.6142, -0.9977,  0.7128, -0.7248, -0.6046]]])\n",
      "Đầu ra layernorm của input:\n",
      "tensor([[[-2.0079e-01, -6.2196e-01,  3.1625e-01, -7.0481e-01,  6.2712e+00],\n",
      "         [-1.5733e-01,  5.5767e-01,  8.3424e-01,  7.0119e-01,  3.1012e+00],\n",
      "         [-1.9025e-01, -1.0468e+00,  5.6732e-01, -9.7005e-01,  3.6598e+00]],\n",
      "\n",
      "        [[ 6.6266e-02, -1.5684e+00,  5.7131e-01,  2.0431e+00,  1.2424e+00],\n",
      "         [ 2.0592e-02, -2.1871e+00,  3.3727e-01,  7.3941e-02,  5.4586e+00],\n",
      "         [-2.5284e-01, -2.0814e-03,  6.0601e-01,  2.9377e-01,  1.2450e+00]]])\n"
     ]
    }
   ],
   "source": [
    "gamma = torch.randn(d_model)\n",
    "print(f\"gamma:\\n{gamma}\")\n",
    "x_hat = (x - mean) / torch.sqrt(var + 1e-6)\n",
    "print(f\"Chuẩn hóa các giá trị của input:\\n{x_hat}\")\n",
    "ans = gamma * x_hat + gamma\n",
    "print(f\"Đầu ra layernorm của input:\\n{ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79c54316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sublayer: Feed forward network: 2 hidden layer\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        output = self.linear2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "424690fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=6, bias=True)\n"
     ]
    }
   ],
   "source": [
    "d_model = 4\n",
    "d_ff = 6\n",
    "linear1 = nn.Linear(d_model, d_ff)\n",
    "print(linear1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "370ee596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual connection Layernorm\n",
    "class SublayerConnection(nn.Module):\n",
    "    # y = layernorm(x + sublayer(x))\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, sublayer):\n",
    "        output = self.norm(x + self.dropout(sublayer(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7635e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.3, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "dropout = nn.Dropout(0.3)\n",
    "print(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09c90af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "tensor([[[ 1.0276, -0.6607,  1.6914,  2.0618, -0.1686, -0.4777],\n",
      "         [ 1.4260, -0.6819,  0.2424, -0.0052,  0.8084, -0.8598],\n",
      "         [ 0.1272,  1.4094, -0.8495, -0.4998, -0.4231,  0.8964],\n",
      "         [ 1.1959, -1.6187, -1.3929,  0.5662, -1.2227, -0.0459]]])\n",
      "output của x khi đi qua residual + layer normalization:\n",
      "tensor([[[ 0.4049, -1.3846,  0.9127,  1.4354, -0.4727, -0.8957],\n",
      "         [ 1.6320, -1.1942, -0.0676, -0.0407,  0.7970, -1.1265],\n",
      "         [ 0.0808,  1.4333, -1.3950, -0.7915, -0.4190,  1.0914],\n",
      "         [ 1.5844, -0.9524, -1.1510,  0.7987, -0.6787,  0.3991]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# y = layernorm(x + sublayer(x))\n",
    "# Giả sử input x là 1 câu có 4 từ, mỗi từ được mã hóa thành vector 6 chiều\n",
    "# ma trận embedding\n",
    "x = torch.randn(1, 4, 6)\n",
    "print(f\"input:\\n{x}\")\n",
    "sublayer = FeedForward(d_model=6, d_ff=10)\n",
    "sublayer_connection = SublayerConnection(6)\n",
    "y = sublayer_connection(x, sublayer)\n",
    "print(f\"output của x khi đi qua residual + layer normalization:\\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd7631",
   "metadata": {},
   "source": [
    "Cài đặt MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c398eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # softmax theo hàng\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x/np.sum(e_x, axis=-1, keepdims=True)\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    # Q: (n, d_k) - n query vectors\n",
    "    # K: (m, d_k) - m key vectors\n",
    "    # V: (m, d_v) - m value vectors\n",
    "    # Lấy ra d_k cho bước tính căn bậc 2\n",
    "    d_k = Q.shape[1]\n",
    "    # Tính score\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    # Tính trọng số attention\n",
    "    weights = softmax(scores)\n",
    "    # Tổ hợp tuyến tính với V\n",
    "    output = np.matmul(weights, V)\n",
    "    print(\"DEBUG - return:\", output.shape, weights.shape)\n",
    "    return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f2de02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        # d_model phải chia hết cho num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        # Khởi tạo ma trận trọng số\n",
    "        self.W_Q = np.random.randn(d_model, d_model)\n",
    "        self.W_K = np.random.randn(d_model, d_model)\n",
    "        self.W_V = np.random.randn(d_model, d_model)\n",
    "        # Ma trận trọng số được sử dụng cuối cùng\n",
    "        self.W_O = np.random.randn(d_model, d_model)\n",
    "    def split_heads(self, x):\n",
    "        # x: (seq_len, d_model)\n",
    "        # return: (num_heads, seq_len, d_k)\n",
    "        seq_len = x.shape[0]\n",
    "        # Tách X thành 1 mảng gồm seq_len ma trận, mỗi ma trận có num_heads hàng, d_k cột\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "    def combine_heads(self, x):\n",
    "        # x: (num_heads, seq_len, d_k)\n",
    "        # return: (seq_len, d_model)\n",
    "        num_heads, seq_len, d_k = x.shape\n",
    "        x = x.transpose(1, 0, 2).reshape(seq_len, self.d_model)\n",
    "    def forward(self, Q, K, V):\n",
    "        # linear projection\n",
    "        Q_proj = Q @ self.W_Q\n",
    "        K_proj = K @ self.W_K\n",
    "        V_proj = V @ self.W_V\n",
    "        # Chia thành nhiều head\n",
    "        Q_heads = self.split_heads(Q_proj)\n",
    "        K_heads = self.split_heads(K_proj)\n",
    "        V_heads = self.split_heads(V_proj)\n",
    "        # Attention trên từng head\n",
    "        head_outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            out, weights = scaled_dot_product_attention(Q_heads[i], K_heads[i], V_heads[i])\n",
    "            head_outputs.append(out)\n",
    "         # Chuyển đổi sang numpy\n",
    "        head_outputs = np.array(head_outputs)\n",
    "        #Ghép lại các head\n",
    "        concat_output = self.combine_heads(head_outputs)\n",
    "        #Linear cuối\n",
    "        output = np.matmul(concat_output, self.W_O)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0aa70c",
   "metadata": {},
   "source": [
    "Pipeline của Encoder trong transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebc5ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class EncoderLayer:\n",
    "    def __init__(self, d_model, d_ff, num_heads, dropout=0.1):\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        # Residual + layer Normalize (2 lần: sau MHA và sau FFN)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # self attention + residual và layer norm\n",
    "        attn_ouput = self.self_attn.forward(x, x, x)\n",
    "        x = self.norm(x + attn_ouput)\n",
    "\n",
    "        # ffn + residual và layer norm\n",
    "        ff_output = self.feed_forward.forward(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3e0e237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "[[ 0.06882774  0.82809309  1.06931332 -1.06252672 -1.53268067  1.03814128\n",
      "  -0.43039354 -2.2288991   0.80210487]\n",
      " [ 0.07738696  1.0420496   1.76247743 -0.58500801  1.87652543  0.58369817\n",
      "  -0.33101837  0.85158475  0.58001963]\n",
      " [ 1.33636926  1.05904713 -0.35722422  1.69440999 -0.32165112 -0.95539771\n",
      "   0.90607909 -0.05959185 -0.98546733]\n",
      " [-0.17041403  1.03521561 -0.90743032  1.31258955 -0.07040159 -1.56574003\n",
      "  -0.50335999 -0.3210434  -1.08486514]\n",
      " [-0.58303137  0.77650346 -0.51738077 -0.06008071  0.2595841   1.00080186\n",
      "   0.81948858  1.12151495  0.95443576]]\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_model = 5, 9\n",
    "d_ff = 16\n",
    "num_heads = 3\n",
    "# input\n",
    "x = np.random.randn(seq_len, d_model)\n",
    "print(f\"input:\\n{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "236f51a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m EncoderLayer(d_model, d_ff, num_heads\u001b[38;5;241m=\u001b[39mnum_heads)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[46], line 12\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# self attention + residual và layer norm\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     attn_ouput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x \u001b[38;5;241m+\u001b[39m attn_ouput)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# ffn + residual và layer norm\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[45], line 37\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, Q, K, V)\u001b[0m\n\u001b[0;32m     35\u001b[0m head_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads):\n\u001b[1;32m---> 37\u001b[0m     out, weights \u001b[38;5;241m=\u001b[39m scaled_dot_product_attention(\u001b[43mQ_heads\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, K_heads[i], V_heads[i])\n\u001b[0;32m     38\u001b[0m     head_outputs\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[0;32m     39\u001b[0m  \u001b[38;5;66;03m# Chuyển đổi sang numpy\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# encoder layer\n",
    "encoder_layer = EncoderLayer(d_model, d_ff, num_heads=num_heads)\n",
    "# Forward\n",
    "output = encoder_layer.forward(x)\n",
    "print(f\"output:\\n{output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duong_kernel_2",
   "language": "python",
   "name": "duong_kernel_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
