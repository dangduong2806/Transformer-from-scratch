{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d336d6e3",
   "metadata": {},
   "source": [
    "Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8082910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def positional_encoding(max_len, d_model):\n",
    "    # max_len: độ dài tối đa của 1 chuỗi\n",
    "    # d_model: kích thước embedding\n",
    "    pe = np.zeros((max_len, d_model))\n",
    "    for pos in range(max_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            angle = pos / (10000 ** (i / d_model))\n",
    "            # Vị trí chẵn\n",
    "            pe[pos, i] = math.sin(angle)\n",
    "            # Vị trí lẻ\n",
    "            pe[pos, i+1] = math.cos(angle)\n",
    "    return pe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f751a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma trận ban đầu:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "------------------\n",
      "ma trận positional encoding:\n",
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.84147098  0.54030231  0.00999983  0.99995   ]\n",
      " [ 0.90929743 -0.41614684  0.01999867  0.99980001]\n",
      " [ 0.14112001 -0.9899925   0.0299955   0.99955003]\n",
      " [-0.7568025  -0.65364362  0.03998933  0.99920011]]\n",
      "--------------------------\n",
      "vector PE cho vị trí thứ: 0 là:\n",
      "[0. 1. 0. 1.]\n",
      "------------------------------\n",
      "vector PE cho vị trí thứ: 1 là:\n",
      "[0.84147098 0.54030231 0.00999983 0.99995   ]\n",
      "------------------------------\n",
      "vector PE cho vị trí thứ: 2 là:\n",
      "[ 0.90929743 -0.41614684  0.01999867  0.99980001]\n",
      "------------------------------\n",
      "vector PE cho vị trí thứ: 3 là:\n",
      "[ 0.14112001 -0.9899925   0.0299955   0.99955003]\n",
      "------------------------------\n",
      "vector PE cho vị trí thứ: 4 là:\n",
      "[-0.7568025  -0.65364362  0.03998933  0.99920011]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "max_len = 5\n",
    "d_model = 4\n",
    "pe = np.zeros((5, 4))\n",
    "print(f\"ma trận ban đầu:\\n{pe}\")\n",
    "print(f\"------------------\")\n",
    "for pos in range(max_len):\n",
    "    for i in range(0, d_model, 2):\n",
    "        angle = pos / (10000 ** (i / d_model))\n",
    "        pe[pos][i] = math.sin(angle)\n",
    "        pe[pos][i+1] = math.cos(angle)\n",
    "print(f\"ma trận positional encoding:\\n{pe}\")\n",
    "print(f\"--------------------------\")\n",
    "for i in range(max_len):\n",
    "    print(f\"vector PE cho vị trí thứ: {i} là:\\n{pe[i]}\")\n",
    "    print(f\"------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51dc3c",
   "metadata": {},
   "source": [
    "Sau khi tính PE, ta cộng vào embedding ban đầu: X' = X + PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a83c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector embedding của input:\n",
      "[[-0.21644298  0.78184617  1.0156912  -0.8887838 ]\n",
      " [ 0.61199426 -0.04717827 -0.1691698  -0.02770232]\n",
      " [ 0.61060641 -1.36513898  0.47786632 -0.0236784 ]\n",
      " [-0.62626462  0.1456579   0.42518529  0.29744258]\n",
      " [ 0.76555819 -0.09552212  0.39565961  0.81810394]]\n",
      "---------------------------------\n",
      "vector x khi thêm positional encoding:\n",
      "[[-0.21644298  1.78184617  1.0156912   0.1112162 ]\n",
      " [ 1.45346524  0.49312403 -0.15916996  0.97224768]\n",
      " [ 1.51990383 -1.78128582  0.49786499  0.97612161]\n",
      " [-0.48514461 -0.84433459  0.45518079  1.29699262]\n",
      " [ 0.00875569 -0.74916574  0.43564894  1.81730405]]\n"
     ]
    }
   ],
   "source": [
    "# Giả sử vector X ban đầu có giá trị:\n",
    "x = np.random.randn(max_len, d_model)\n",
    "print(f\"vector embedding của input:\\n{x}\")\n",
    "x = x + pe\n",
    "print(f\"---------------------------------\")\n",
    "print(f\"vector x khi thêm positional encoding:\\n{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edffcdde",
   "metadata": {},
   "source": [
    "Multihead Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf922d",
   "metadata": {},
   "source": [
    "Residual and Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "013da2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# Layer normalization\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # gamma và beta\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased= False)\n",
    "\n",
    "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x_hat + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d09530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma1:\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], requires_grad=True)\n",
      "gamma2:\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "---------------------------\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "d_model = 5\n",
    "gamma1 = nn.Parameter(torch.zeros(d_model))\n",
    "gamma2 = torch.zeros(d_model)\n",
    "print(f\"gamma1:\\n{gamma1}\")\n",
    "print(f\"gamma2:\\n{gamma2}\")\n",
    "print(f\"---------------------------\")\n",
    "matrix = torch.zeros(3,4)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b39125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1762,  1.4007,  0.5833,  0.5168,  0.5854],\n",
      "         [ 0.4201,  0.3855,  0.1350, -1.1465, -1.2245],\n",
      "         [-0.7071,  0.3370, -1.4839, -0.9447,  0.2987]],\n",
      "\n",
      "        [[ 1.8559, -0.0228, -0.4383, -0.1640, -1.1650],\n",
      "         [ 0.8335, -1.3620,  0.5914, -0.1554,  0.0164],\n",
      "         [ 0.3649, -0.1550, -1.7266,  1.4740, -1.5907]]])\n",
      "Trung bình của từng hàng: \n",
      "tensor([[[ 0.6525],\n",
      "         [-0.2861],\n",
      "         [-0.5000]],\n",
      "\n",
      "        [[ 0.0132],\n",
      "         [-0.0152],\n",
      "         [-0.3267]]])\n",
      "Phương sai của từng hàng:\n",
      "tensor([[[0.1629],\n",
      "         [0.5496],\n",
      "         [0.5094]],\n",
      "\n",
      "        [[1.0041],\n",
      "         [0.5845],\n",
      "         [1.4615]]])\n"
     ]
    }
   ],
   "source": [
    "# Tạo input gồm 2 câu batch_size = 2\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 5\n",
    "x = torch.randn(batch_size,seq_len, d_model)\n",
    "print(x)\n",
    "mean = x.mean(dim=-1, keepdim=True)\n",
    "print(f\"Trung bình của từng hàng: \\n{mean}\")\n",
    "var = x.var(dim = -1, keepdim=True, unbiased=False)\n",
    "print(f\"Phương sai của từng hàng:\\n{var}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7337264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma:\n",
      "tensor([ 1.0498,  2.7358, -0.9048,  0.3726, -1.2780])\n",
      "Chuẩn hóa các giá trị của input:\n",
      "tensor([[[-1.1800,  1.8540, -0.1714, -0.3363, -0.1663],\n",
      "         [ 0.9526,  0.9059,  0.5680, -1.1606, -1.2658],\n",
      "         [-0.2902,  1.1727, -1.3785, -0.6231,  1.1190]],\n",
      "\n",
      "        [[ 1.8390, -0.0359, -0.4505, -0.1768, -1.1758],\n",
      "         [ 1.1101, -1.7615,  0.7934, -0.1834,  0.0414],\n",
      "         [ 0.5720,  0.1420, -1.1579,  1.4895, -1.0456]]])\n",
      "Đầu ra layernorm của input:\n",
      "tensor([[[-0.1890,  7.8081, -0.7497,  0.2473, -1.0655],\n",
      "         [ 2.0498,  5.2141, -1.4187, -0.0599,  0.3397],\n",
      "         [ 0.7452,  5.9441,  0.3424,  0.1405, -2.7081]],\n",
      "\n",
      "        [[ 2.9805,  2.6375, -0.4972,  0.3067,  0.2247],\n",
      "         [ 2.2152, -2.0833, -1.6227,  0.3043, -1.3309],\n",
      "         [ 1.6504,  3.1243,  0.1429,  0.9276,  0.0582]]])\n"
     ]
    }
   ],
   "source": [
    "gamma = torch.randn(d_model)\n",
    "print(f\"gamma:\\n{gamma}\")\n",
    "x_hat = (x - mean) / torch.sqrt(var + 1e-6)\n",
    "print(f\"Chuẩn hóa các giá trị của input:\\n{x_hat}\")\n",
    "ans = gamma * x_hat + gamma\n",
    "print(f\"Đầu ra layernorm của input:\\n{ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79c54316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sublayer: Feed forward network: 2 hidden layer\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        output = self.linear2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "424690fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=6, bias=True)\n"
     ]
    }
   ],
   "source": [
    "d_model = 4\n",
    "d_ff = 6\n",
    "linear1 = nn.Linear(d_model, d_ff)\n",
    "print(linear1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370ee596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual connection Layernorm\n",
    "class SublayerConnection(nn.Module):\n",
    "    # y = layernorm(x + sublayer(x))\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, sublayer):\n",
    "        output = self.norm(x + self.dropout(sublayer(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7635e6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.3, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "dropout = nn.Dropout(0.3)\n",
    "print(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09c90af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "tensor([[[-0.5415, -0.1180,  1.8667, -1.2576, -1.2498,  1.0872],\n",
      "         [-1.3945, -0.5030, -0.3373, -1.9713, -1.6880, -0.5745],\n",
      "         [-0.5466, -0.8399, -0.2512, -1.1144,  0.2116, -0.9814],\n",
      "         [-1.3205,  0.8312,  1.1146,  1.0951,  0.3491,  0.1112]]])\n",
      "output của x khi đi qua residual + layer normalization:\n",
      "tensor([[[-0.6559,  0.0169,  1.6040, -1.0853, -0.8916,  1.0119],\n",
      "         [-0.8976,  0.7418,  1.3452, -1.0599, -0.9874,  0.8579],\n",
      "         [-0.8217, -0.2124,  1.6676, -1.2697,  0.9030, -0.2669],\n",
      "         [-2.0583,  0.8976,  0.0138,  0.9534,  0.2188, -0.0253]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# y = layernorm(x + sublayer(x))\n",
    "# Giả sử input x là 1 câu có 4 từ, mỗi từ được mã hóa thành vector 6 chiều\n",
    "# ma trận embedding\n",
    "x = torch.randn(1, 4, 6)\n",
    "print(f\"input:\\n{x}\")\n",
    "sublayer = FeedForward(d_model=6, d_ff=10)\n",
    "sublayer_connection = SublayerConnection(6)\n",
    "y = sublayer_connection(x, sublayer)\n",
    "print(f\"output của x khi đi qua residual + layer normalization:\\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd7631",
   "metadata": {},
   "source": [
    "Cài đặt MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c398eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # softmax theo hàng\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x/np.sum(e_x, axis=-1, keepdims=True)\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    # Q: (n, d_k) - n query vectors\n",
    "    # K: (m, d_k) - m key vectors\n",
    "    # V: (m, d_v) - m value vectors\n",
    "    # Lấy ra d_k cho bước tính căn bậc 2\n",
    "    d_k = Q.shape[1]\n",
    "    # Tính score\n",
    "    scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
    "    # Tính trọng số attention\n",
    "    weights = softmax(scores)\n",
    "    # Tổ hợp tuyến tính với V\n",
    "    output = np.matmul(weights, V)\n",
    "    print(\"DEBUG - return:\", output.shape, weights.shape)\n",
    "    return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4f050e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - return: (4, 4) (4, 4)\n",
      "[[-0.93752795  0.25275933 -0.46457028 -0.09316837]\n",
      " [-0.10914321  0.58239979 -1.60843841  0.39956429]\n",
      " [-0.28380045  0.06269882 -0.64974329 -0.25192324]\n",
      " [-0.08112379  0.64395618 -1.55195868  0.356688  ]]\n"
     ]
    }
   ],
   "source": [
    "q = np.random.randn(4, 4)\n",
    "k = np.random.randn(4, 4)\n",
    "v = np.random.randn(4, 4)\n",
    "ans, weight = scaled_dot_product_attention(q, k , v)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f2de02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        # d_model phải chia hết cho num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        # Khởi tạo ma trận trọng số\n",
    "        self.W_Q = np.random.randn(d_model, d_model)\n",
    "        self.W_K = np.random.randn(d_model, d_model)\n",
    "        self.W_V = np.random.randn(d_model, d_model)\n",
    "        # Ma trận trọng số được sử dụng cuối cùng\n",
    "        self.W_O = np.random.randn(d_model, d_model)\n",
    "    def split_heads(self, x):\n",
    "        # x: (seq_len, d_model)\n",
    "        # return: (num_heads, seq_len, d_k)\n",
    "        seq_len = x.shape[0]\n",
    "        # Tách X thành 1 mảng gồm seq_len ma trận, mỗi ma trận có num_heads hàng, d_k cột\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        x = x.transpose(1, 0, 2)\n",
    "        return x\n",
    "    def combine_heads(self, x):\n",
    "        # x: (num_heads, seq_len, d_k)\n",
    "        # return: (seq_len, d_model)\n",
    "        num_heads, seq_len, d_k = x.shape\n",
    "        x = x.transpose(1, 0, 2).reshape(seq_len, self.d_model)\n",
    "        return x\n",
    "    def forward(self, Q, K, V):\n",
    "        # linear projection\n",
    "        Q_proj = Q @ self.W_Q\n",
    "        K_proj = K @ self.W_K\n",
    "        V_proj = V @ self.W_V\n",
    "        # Chia thành nhiều head\n",
    "        Q_heads = self.split_heads(Q_proj)\n",
    "        K_heads = self.split_heads(K_proj)\n",
    "        V_heads = self.split_heads(V_proj)\n",
    "        # Attention trên từng head\n",
    "        head_outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            out, weights = scaled_dot_product_attention(Q_heads[i], K_heads[i], V_heads[i])\n",
    "            head_outputs.append(out)\n",
    "         # Chuyển đổi sang numpy\n",
    "        head_outputs = np.array(head_outputs)\n",
    "        #Ghép lại các head\n",
    "        concat_output = self.combine_heads(head_outputs)\n",
    "        #Linear cuối\n",
    "        output = np.matmul(concat_output, self.W_O)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0aa70c",
   "metadata": {},
   "source": [
    "Pipeline của Encoder trong transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebc5ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class EncoderLayer:\n",
    "    def __init__(self, d_model, d_ff, num_heads, dropout=0.1):\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        # Residual + layer Normalize (2 lần: sau MHA và sau FFN)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # self attention + residual và layer norm\n",
    "        attn_ouput = self.self_attn.forward(x, x, x)\n",
    "        x = torch.from_numpy(x).float()\n",
    "        attn_ouput = torch.from_numpy(attn_ouput).float()\n",
    "        x = self.norm1(x + attn_ouput)\n",
    "\n",
    "        # ffn + residual và layer norm\n",
    "        ff_output = self.feed_forward.forward(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3e0e237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "[[ 0.50802976  0.23444936 -0.59159537 -0.36601918  1.67046587  0.75439201\n",
      "   0.41214895 -1.89675187  1.12272172]\n",
      " [-0.09154087 -1.23318577  0.60780356 -1.4386419   0.85144728 -0.42111865\n",
      "   1.38579297 -1.59434061  0.61999749]\n",
      " [ 0.28131394  1.45682223  0.26959276 -0.958653   -0.9690516  -0.67968528\n",
      "  -0.15747996 -0.10191642 -0.85026526]\n",
      " [ 0.59283022 -0.34514973 -1.35365388 -0.12678413  0.55361445 -0.79478839\n",
      "   0.83515877  0.38724302 -0.68556264]\n",
      " [ 1.10614352  0.65093095 -1.79333279 -0.30305034  0.14238003 -1.6305611\n",
      "   0.03470773  0.87840114  0.16645505]]\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_model = 5, 9\n",
    "d_ff = 16\n",
    "num_heads = 3\n",
    "# input\n",
    "x = np.random.randn(seq_len, d_model)\n",
    "print(f\"input:\\n{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "236f51a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - return: (5, 3) (5, 5)\n",
      "DEBUG - return: (5, 3) (5, 5)\n",
      "DEBUG - return: (5, 3) (5, 5)\n",
      "output:\n",
      "tensor([[ 0.0910,  0.0656, -0.5387, -1.2596,  0.4712,  1.7055, -0.2591,  1.2539,\n",
      "         -1.5299],\n",
      "        [ 1.1639, -0.6475, -1.4581,  0.0252,  0.2831,  0.4773, -0.0754,  1.6583,\n",
      "         -1.4267],\n",
      "        [ 0.9327, -0.8510, -1.4318,  0.3775, -0.3850,  0.2372,  0.3524,  1.8989,\n",
      "         -1.1308],\n",
      "        [-1.3812,  0.8436,  0.7653, -0.9898,  0.4205,  0.7697,  0.7205, -1.7717,\n",
      "          0.6230],\n",
      "        [ 1.0025, -0.8041, -1.6736,  1.5121,  0.1125, -1.3164,  0.5469,  0.1999,\n",
      "          0.4203]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# encoder layer\n",
    "encoder_layer = EncoderLayer(d_model, d_ff, num_heads=num_heads)\n",
    "# Forward\n",
    "output = encoder_layer.forward(x)\n",
    "print(f\"output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1425725b",
   "metadata": {},
   "source": [
    "Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "279fb771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Attention\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def masked_scaled_dot_product_attention(Q, K, V, mask= None):\n",
    "    # Q: (batch, seq_len, d_k)\n",
    "    # K: (batch, seq_len, d_k)\n",
    "    # V: (batch, seq_len, d_v)\n",
    "    d_k = Q.size(-1)\n",
    "    # Tính scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype= torch.float32))\n",
    "    if mask is not None:\n",
    "        # mask = 0 => j <= i\n",
    "        # mask = -inf => j > i\n",
    "        scores = scores + mask\n",
    "    # softmax theo chiều seq_len\n",
    "    weights = F.softmax(scores, dim= -1)\n",
    "    # (batch, seq_len, d_v)\n",
    "    output = torch.matmul(weights, V)\n",
    "    return output, weights\n",
    "def generate_subsequent_mask(seq_len):\n",
    "    # Tạo mask tam giác dưới: (seq_len, seq_len)\n",
    "    # 1. Tạo ma trận toàn 1 (seq_len, seq_len)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagnol= 1)\n",
    "    # 2. Các phần tử bằng 1 được thay bằng -inf\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25f1b65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k, d_v = 4, 8\n",
    "batch_size, seq_len = 2, 5\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_v)\n",
    "Q.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duong_kernel_2",
   "language": "python",
   "name": "duong_kernel_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
